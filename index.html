<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <!--<link href="https://fonts.googleapis.com/css?family=Slabo+27px" rel="stylesheet">-->
  <link href="https://fonts.googleapis.com/css?family=PT+Sans" rel="stylesheet">
  <title>KeypointNet</title>
  <style type="text/css">
    body {
      margin: 0px;
      background-color: #F3F3F3;
      font-family: 'PT Sans', 'Arial';
    }

    h1 {
      text-align: center;
      color: #444444;
      font-size: 36px;
      margin-top:50px;
      margin-bottom: 0px;
    }
    h2 {
      margin-top: 120px;
      margin-bottom: 10px;
      color: #FFCF43;
      font-size: 30px;
    }

    #headerdiv {
      width: 1150px;
      margin: auto;
    }
    #maincontent {
      width: 1000px;
      margin: auto;
      height: 340px;
    }

    #suppcontent {
      padding-top: 0px;
      background-color: black;
      color: #BBBBBB;
      font-size: 16px;
    }
    
    #content {
      width: 1000px;
      margin: auto;
    }
    td {
      vertical-align: top;
      text-align: center;
    }
    .desc {
      color: #888888;
    }
    .center {
      display: block;
      width: 100%;
      text-align: center;
    }
    .nips {
      text-align: center;
      margin-top: 0px;
      font-size: 28px;
      margin-bottom: 20px;
    }
    .nips a {
      color: #777777;
    }
    .google {
      text-align: center;
      margin-top: 2px;
      font-size: 23px;
    }
    .google a {
      color: #204c87;
    }
    .center {
      text-align: center;
    }
    .authors {
      text-align: center;
    }
    .author {
      margin-right: 58px;
      font-size: 21px;
    }
    .author a {
      color: #3376cb;
    }
    #abstract {
      line-height: 1.3;
      width: 1000px;
      font-size: 16px;
    }
    h2 {
      color: #666666;
      margin-top: 70px;
    }


    h1.supp {
      padding-top: 40px;
      margin-top: 0;
      color:#aaaaaa;
      font-size:40px;
      margin-bottom: 20px;
    }
    h2.abstract {
      color: #555555;
      margin-top: 0px;
    }

    #files {
      color: #555555;
      margin-top: 20px;
      font-size: 25px;
    }

    #files a {
      text-decoration: None;
      color: #3376cb;
    }

    a {
      text-decoration: none;
    }
  </style>
</head>
<body>
  <div id="headerdiv">
  <h1>
  Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning
  </h1>
  <div class="nips"><a href="https://nips.cc/">NIPS 2018 (Oral)</a></div>
</div>
  <div id="maincontent">
  <div class="authors">
  <span class="author"> <a href="http://www.supasorn.com">Supasorn Suwajanakorn</a></span>
  <span class="author"> <a href="http://www.cs.cornell.edu/~snavely/">Noah Snavely</a></span>
  <span class="author"> <a href="https://cims.nyu.edu/~tompson/">Jonathan Tompson</a></span>
  <span class="author"> <a href="https://norouzi.github.io/">Mohammad Norouzi</a></span>
  </div>
  
  <div class="google"><a href="https://ai.google/">Google AI</a></div>

  <div id="abstract">
    <h2 class="abstract"> Abstract </h2>
    This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific 3D keypoints, along with their detectors.  Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task.  We demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. Our model discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category. Importantly, we find that our end-to-end framework using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation.  The discovered 3D keypoints on the car, chair, and plane categories of <a href="https://www.shapenet.org/">ShapeNet</a> are visualized below.
    <div id="files" class="center" style="margin-top: 10px">
      [ <a href="keypointnet_neurips.pdf">Paper</a> ]&nbsp;&nbsp;&nbsp;&nbsp;
      [ <a href="https://github.com/tensorflow/models/tree/archive/research/keypointnet">Code</a> ]&nbsp;&nbsp;&nbsp;&nbsp;
      [ <a href="keypointnet_trained_model_car.zip">Pretrained Model</a> ]
    </div>
  </div>
</div>
<div id="suppcontent">
  <div style="height: 20px; background-color: #414141;"></div>
  <div id="content">
    <!--<h1 class="supp">3D Keypoints</h1>-->
  <h2 style="margin-top: 30px;">Keypoint prediction from any angles</h2>
  Results with more viewing variations (Supplement to Figure 3 in the paper). This is frame-by-frame prediction with no temporal constraints.<br/>
  <span class="center"><img src="html_files/plane_spin.gif" width="180"><img src="html_files/chair_spin.gif" width="180"><img src="html_files/car_spin.gif" width="180"></span>
  <h2>Chairs</h2>
  Test chairs from ShapeNet. This is a frame-by-frame keypoint prediction on each animation frame. No temporal information is used.<br>
  We show how the network is able to utilize the same keypoints across object instances and consistently predict keypoints across viewing angles, even when parts are occluded such as the back legs.
<video width="900" loop autoplay muted>
  <source src="html_files/chairs.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
  <h2>Planes</h2>
  Test planes from ShapeNet. Notice failure cases in the bottom row where the orientation network fails to predict the correct orientation of a few planes with very unusual wing shapes. <br>
<video width="900" loop autoplay muted>
  <source src="html_files/planes.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
  <h2>Cars</h2>
  Test cars from ShapeNet. Notice failure cases on the bottom row: The second car is mostly black which is the same as the background. The third car looks very symmetric that the predicted orientation is sometimes reversed.<br>
<video width="900" loop autoplay muted>
  <source src="html_files/cars.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
  <h2>Ablation Study</h2>
  We present an ablation study for the primary losses as well as how their weights affect the results.<br>
  <table>
    <tr>
      <td><img src="html_files/ab_base.gif" width="180"><br>Baseline (12 keypoints)</td>
      <td><img src="html_files/ab_closs0.gif" width="180"><br>No multiview consistency<br><span class="desc">Notice how points are no longer stationary.</span></td>
      <td><img src="html_files/ab_peloss0.gif" width="180"><br>No relative pose loss<br><span class="desc">Points are concentrated at the center, but are still separated from separation loss</span></td>
      <td><img src="html_files/ab_noise0.2.gif" width="180"><br>More noise in relative pose loss<br><span class="desc">This encourages points to be more robust to estimation error by spreading them out further.</span></td>
      <!--<td><img src="html_files/ab_noise0.gif" width="180"><br>No noise<br><span class="desc">Notice how points are no longer stationary.</span></td>-->
      <td><img src="html_files/ab_oncar.gif" width="180"><br>No silhouette loss<br><span class="desc">This causes the points to lie outside the car.</span></td>
    </tr>
  </table>
<h2>Varying number of keypoints</h2>
We trained the network with varying number of keypoints [3, 5, 8, 10, 15, 20]. The network starts by discovering the most prominent components such as the head and wings, then gradually tracks more parts as the number increases. Colors do not correspond across results as they're independent. <br>
<img src="./planekps/anim.gif" width="165"><img src="./planekps/anim(1).gif" width="165"><img src="./planekps/anim(2).gif" width="165"><img src="./planekps/anim(3).gif" width="165"><img src="./planekps/anim(4).gif" width="165"><img src="./planekps/anim(5).gif" width="165">
  <h2>Results on deformed object</h2>
  To evaluate the robustness of these keypoints under shape variations, and whether the network uses local features to detect local parts as opposed to placing keypoints on a regular rigid structure, we test our network on a non-rigidly deformed car. Here the network is able to predict where the wheels are and the overall deformation of the car structure.<br>
  <span class="center"><img src="html_files/expand.gif" width="180"><img src="html_files/wig1.gif" width="180"><img src="html_files/wig2.gif" width="180"></span>
  <h2>3D Visualization of keypoints</h2>
  We visualize the positions of the predicted 3D keypoints by projecting them back to the 3D mesh of the following car. We show results for all 120 frames used to generate the animation. The frustrum indicates the camera's direction. (Our algorithm never has access to the 3D and take as input a single image. This is only for visualization.) The variances of these keypoint locations are reported in Table 1 in the paper. <br>
  <span class="center"><img src="html_files/car_none.gif" width="180"></span><br>
  
  <br><br>
<video width="1100" controls loop autoplay muted>
  <source src="html_files/vis3d.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

  <h2>Proof-of-concept results on real images</h2>
  We train our network by adding random backgrounds to our rendered training examples. Surprisingly, such a simple modification allows the network to predict keypoints on some of the Imagenet's cars. We show a few hand-picked results and some failure modes on the right. The network especially has difficulties dealing with large perspective distortion and cars that have strong patterns or specular highlights.<br><br>
  <span class="center"><img src="html_files/realresults2.png" width="1000"></span><br>
  <br><br>
</div>
</div>
</body>
</html>
